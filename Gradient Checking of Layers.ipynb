{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradient checking of Layers\n",
    "\n",
    "Verify the correctness of implementation using Gradient checks provided in CS231 2nd assignment.\n",
    "\n",
    "1. **Probably Wrong**: relative error > 1e-2 \n",
    "2. **Something not right** :1e-2 > relative error > 1e-4 \n",
    "3. **Okay for objectives with kinks**: 1e-4 > relative error, if no kinks then too high\n",
    "4. **Probably Right**: relative error < 1e-7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import *\n",
    "from nonlinearity import ReLU\n",
    "from utils import numerical_gradient_array,eval_numerical_gradient,rel_error,load_mnist,load_cifar10\n",
    "from time import time\n",
    "from loss import SoftmaxLoss,regularization,delta_regularization\n",
    "from nnet import NeuralNet\n",
    "from solver import sgd,sgd_momentum,adam\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer\n",
    "\n",
    "Perform numerical grdient checking for verifying the implementation of convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "The difference of correct_out and out should be around 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward pass of Conv Layer\n",
      "Difference:  2.21214764967e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "c_layer = Conv((3,4,4),n_filter=3,h_filter=4,w_filter=4,stride=2,padding=1)\n",
    "c_layer.W = w\n",
    "c_layer.b = b.reshape(-1,1)\n",
    "\n",
    "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "out = c_layer.forward(x)\n",
    "\n",
    "error = rel_error(out,correct_out)\n",
    "print(\"Testing forward pass of Conv Layer\")\n",
    "print(\"Difference: \",error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "The errors for gradients should be around 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of Conv Layer\n",
      "dX error:  5.04631083003e-09\n",
      "dW error:  1.21968306724e-08\n",
      "db error:  1.82009326877e-11\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,).reshape(-1,1)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "\n",
    "c_layer = Conv((3,5,5),n_filter=2,h_filter=3,w_filter=3,stride=1,padding=1)\n",
    "c_layer.W = w\n",
    "c_layer.b = b\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x: c_layer.forward(x), x, dout)\n",
    "dw_num = numerical_gradient_array(lambda w: c_layer.forward(x), w, dout)\n",
    "db_num = numerical_gradient_array(lambda b: c_layer.forward(x), b, dout)\n",
    "\n",
    "out = c_layer.forward(x)\n",
    "dx,grads = c_layer.backward(dout)\n",
    "dw,db = grads\n",
    "\n",
    "print(\"Testing backward pass of Conv Layer\")\n",
    "print(\"dX error: \",rel_error(dx,dx_num))\n",
    "print(\"dW error: \",rel_error(dw,dw_num))\n",
    "print(\"db error: \",rel_error(db,db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxpool Layer\n",
    "\n",
    "Perform gradient check for maxpool layer and verify correctness of its implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "Difference should be around 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.16666651573e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "\n",
    "pool = Maxpool((3,4,4),size=2,stride=2)\n",
    "\n",
    "out = pool.forward(x,)\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "print('Testing max_pool_forward_naive function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "\n",
    "Error should be around 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing bacward pass of Maxpool layer\n",
      "dX error:  3.27564681622e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "\n",
    "pool = Maxpool((2,8,8),size=2,stride=2)\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x: pool.forward(x), x, dout)\n",
    "\n",
    "out = pool.forward(x)\n",
    "dx,_ = pool.backward(dout)\n",
    "\n",
    "print('Testing bacward pass of Maxpool layer')\n",
    "print('dX error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer\n",
    "Error should be around 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of ReLU layer\n",
      "dX error:  3.27561559317e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 8, 8)\n",
    "\n",
    "r = ReLU()\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x:r.forward(x), x, dout)\n",
    "\n",
    "out = r.forward(x)\n",
    "dx,_ = r.backward(dout)\n",
    "\n",
    "print('Testing backward pass of ReLU layer')\n",
    "print('dX error: ',rel_error(dx,dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv-ReLU-MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu_pool\n",
      "dx error:  6.88787711827e-09\n",
      "dw error:  9.29133425491e-10\n",
      "db error:  6.67766526233e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2, 3, 16, 16)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,).reshape(-1,1)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "\n",
    "c = Conv((3,16,16),n_filter=3,h_filter=3,w_filter=3,stride=1,padding=1)\n",
    "c.W, c.b = w, b\n",
    "r = ReLU()\n",
    "m = Maxpool(c.out_dim,size=2,stride=2)\n",
    "\n",
    "def conv_relu_pool_forward(c,r,m,x):\n",
    "    c_out = c.forward(x)\n",
    "    r_out = r.forward(c_out)\n",
    "    m_out = m.forward(r_out)\n",
    "    return m_out\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x: conv_relu_pool_forward(c,r,m,x), x, dout)\n",
    "dw_num = numerical_gradient_array(lambda w: conv_relu_pool_forward(c,r,m,x), w, dout)\n",
    "db_num = numerical_gradient_array(lambda b: conv_relu_pool_forward(c,r,m,x), b, dout)\n",
    "\n",
    "m_dx,_ = m.backward(dout)\n",
    "r_dx,_ = r.backward(m_dx)\n",
    "dx,grads = c.backward(r_dx)\n",
    "dw,db = grads\n",
    "\n",
    "\n",
    "print('Testing conv_relu_pool')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.49834967  1.70660132  1.91485297]\n",
      " [ 3.25553199  3.5141327   3.77273342]]\n",
      "Testing fully connected forward pass:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim).reshape(1,-1)\n",
    "\n",
    "flat = Flatten()\n",
    "x = flat.forward(x)\n",
    "\n",
    "f = FullyConnected(120,3)\n",
    "f.W,f.b= w,b\n",
    "out = f.forward(x)\n",
    "\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "print(out)\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing fully connected forward pass:')\n",
    "print('difference: ', rel_error(out, correct_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fully connected backward pass:\n",
      "dx error:  1.00870024555e-09\n",
      "dw error:  1.46833423537e-10\n",
      "db error:  1.1317301e-10\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "flat = Flatten()\n",
    "x = flat.forward(x)\n",
    "\n",
    "f = FullyConnected(60,5)\n",
    "f.W,f.b= w,b\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x: f.forward(x), x, dout)\n",
    "dw_num = numerical_gradient_array(lambda w: f.forward(x), w, dout)\n",
    "db_num = numerical_gradient_array(lambda b: f.forward(x), b, dout)\n",
    "\n",
    "dx,grads= f.backward(dout)\n",
    "dw, db = grads\n",
    "# The error should be around 1e-10\n",
    "print('Testing fully connected backward pass:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SoftmaxLoss:\n",
      "loss:  2.30269056333\n",
      "dx error:  8.37370677824e-09\n"
     ]
    }
   ],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: SoftmaxLoss(x,y)[0], x,verbose=False)\n",
    "loss,dx = SoftmaxLoss(x,y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print('Testing SoftmaxLoss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a simple CNN\n",
    "\n",
    "When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about log(C) for C classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  3.50764487292e-08\n",
      "Initial loss (no regularization):  6.03857135481\n"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "num_class = 10\n",
    "X = np.random.randn(N, 3, 10, 10)\n",
    "y = np.random.randint(num_class, size=N)\n",
    "dout = np.random.randn(N, num_class)\n",
    "\n",
    "def make_cnn(X_dim,num_class):\n",
    "    conv = Conv(X_dim,n_filter=3,h_filter=3,w_filter=3,stride=1,padding=1)\n",
    "    relu_conv = ReLU()\n",
    "    maxpool = Maxpool(conv.out_dim,size=2,stride=1)\n",
    "    flat = Flatten()\n",
    "    fc = FullyConnected(np.prod(maxpool.out_dim),num_class)\n",
    "    relu_fc = ReLU()\n",
    "    return [conv,relu_conv,maxpool,flat,fc,relu_fc]\n",
    "\n",
    "cnn = NeuralNet(make_cnn((3,10,10),num_class))\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda X: cnn.forward(X), X, dout)\n",
    "out = cnn.forward(X)\n",
    "dx,grads = cnn.backward(dout)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "loss,dout = SoftmaxLoss(out,y)\n",
    "\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "\n",
    "# # r_loss,out = cnn.forward(X,y,reg=True)\n",
    "# # print('Initial loss (with regularization): ', r_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
